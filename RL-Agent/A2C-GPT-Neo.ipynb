{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from transformers import GPTNeoForCausalLM, GPTNeoModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "generator_model_path = \"./models/pretrain-models\"\n",
    "reward_model_path = \"./models/reward-models\"\n",
    "\n",
    "# === LOAD TOKENIZER ===\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(generator_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === VALUE HEAD WRAPPER ===\n",
    "class GPTNeoWithValueHead(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.base_model(input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        last_token_hidden = hidden_states[:, -1, :]\n",
    "        value = self.value_head(last_token_hidden).squeeze(-1)\n",
    "        return outputs, value\n",
    "\n",
    "# === LOAD GENERATOR MODEL ===\n",
    "base_model = GPTNeoForCausalLM.from_pretrained(generator_model_path).to(device)\n",
    "generator = GPTNeoWithValueHead(base_model).to(device)\n",
    "optimizer = optim.Adam(generator.parameters(), lr=1e-5)\n",
    "\n",
    "# === LOAD REWARD MODEL ===\n",
    "reward_model = GPTNeoModel.from_pretrained(reward_model_path).to(device)\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0946d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE SEQUENCE ===\n",
    "def generate_sequence_with_grad(prompt, max_length=50):\n",
    "    generator.train()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generated = input_ids.clone()\n",
    "    log_probs = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        outputs, _ = generator(generated)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        next_token = dist.sample()\n",
    "        log_prob = dist.log_prob(next_token)\n",
    "        log_probs.append(log_prob)\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    total_log_prob = torch.stack(log_probs).sum()\n",
    "    _, value = generator(generated)\n",
    "    return generated, total_log_prob, value.squeeze(), generated\n",
    "\n",
    "# === GET REWARD ===\n",
    "def get_reward(prompt, response_ids):\n",
    "    text = prompt + tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(**inputs)\n",
    "        last_hidden = outputs.last_hidden_state[:, -1, :]\n",
    "        score = torch.sigmoid(last_hidden.mean(dim=-1))\n",
    "        return score.item()\n",
    "\n",
    "# === A2C UPDATE ===\n",
    "def a2c_update(log_probs, rewards, values):\n",
    "    total_loss = 0.0\n",
    "    for log_prob, reward, value in zip(log_probs, rewards, values):\n",
    "        advantage = reward - value\n",
    "        actor_loss = -log_prob * advantage.detach()\n",
    "        critic_loss = advantage.pow(2)\n",
    "        total_loss += actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING LOOP ===\n",
    "def train_a2c(prompts, epochs=2, batch_size=8):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        for i in tqdm(range(0, len(prompts), batch_size), desc=f\"Epoch {epoch+1}\"):\n",
    "            batch_prompts = prompts[i:i+batch_size]\n",
    "            log_probs, rewards, values = [], [], []\n",
    "\n",
    "            for prompt in batch_prompts:\n",
    "                generated, log_prob, value, generated_ids = generate_sequence_with_grad(prompt)\n",
    "                reward = get_reward(prompt, generated_ids)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(torch.tensor(reward, device=device))\n",
    "                values.append(value)\n",
    "\n",
    "            a2c_update(log_probs, rewards, values)\n",
    "\n",
    "        print(f\"\\n=== GRAD CHECK (Epoch {epoch+1}) ===\")\n",
    "        for name, param in generator.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name} grad mean: {param.grad.mean().item():.6f}\")\n",
    "\n",
    "# === LOAD PROMPTS ===\n",
    "df = pd.read_csv('./dataset/XSS_Dataset-1m.txt', names=[\"prompt\"], on_bad_lines='skip', nrows=1000)\n",
    "prompts = df['prompt'].astype(str).tolist()\n",
    "\n",
    "# === START TRAINING ===\n",
    "train_a2c(prompts, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE ===\n",
    "save_path = \"./models/finetune-models/gpt-neo-a2c-XSS\"\n",
    "generator.base_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
