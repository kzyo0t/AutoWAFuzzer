{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45734629",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "payloads = pd.read_csv('./data/SQLi/sqli.txt', names=[\"payloads\"], nrows=1000000, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ae93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(payloads)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68877ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb5045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.data/attack-grammars/sqli/vocab.json/vocab.json') as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "# Extract the values into a list\n",
    "special_tokens = list(vocab.keys())\n",
    "print(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc85128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cce994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the text and prepare labels\n",
    "    encoding = tokenizer(examples[\"payloads\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"payloads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0729ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c275ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/pretrain-models/gpt-neo-checkpoints\",\n",
    "    per_device_train_batch_size=2,   # Adjust batch size based on your GPU memory1\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./models/pretrain-models/gpt-neo-checkpoints/logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99784b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.save_pretrained('./models/pretrain-models/gpt_neo_1m')\n",
    "tokenizer.save_pretrained('./models/pretrain-models/gpt_neo_1m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
